<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="EN" lang="EN" dir="ltr">
<head profile="http://gmpg.org/xfn/11">
	<link rel="shortcut icon" href="favicon.ico" />

	<title>CORSMAL: Collaborative object recognition, shared manipulation and learning | data</title>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta http-equiv="imagetoolbar" content="no" />
	<meta http-equiv="KeyWords" content="data, datasets, dataset, CORSMAL, robotics, touch, vision, audio, signal processing, human behaviour"/>
	<meta name="image" property="og:image" content="images/CORSMAL_logo.png">
	<link rel="stylesheet" href="css/layout.css" type="text/css" />
	<script type="text/javascript" src="js/jquery-1.4.1.min.js"></script>
	<script type="text/javascript" src="js/jquery.slidepanel.setup.js"></script>
	<script type="text/javascript" src="js/jquery-ui-1.7.2.custom.min.js"></script>
	<script type="text/javascript" src="js/jquery.tabs.setup.js"></script>

<style type="text/css">
	tr:hover {background-color: #f5f5f5;}
	.tg  {border-collapse:collapse;border-spacing:0;}
	.tg td{border-color:black;border-style:solid;border-width:1px;overflow:hidden;padding:10px 5px;word-break:normal;}
		.tg th{border-color:black;border-style:solid;border-width:1px;font-weight:normal;font-size:12.0pt;overflow:hidden;padding:10px 5px;word-break:normal;}
			.tg .tg-km2t{border-color:#ffffff;font-weight:bold;font-size:12.0pt;text-align:left;vertical-align:top}
			.tg .tg-zv4m{border-color:#ffffff;font-size:13.0pt;text-align:left;vertical-align:top}
</style>
	
</head>
<body id="top">
	<div class="wrapper row1">
		<div id="header" class="clear">
			<div class="fl_left">
				<ul>
					<li>
						<p>
							<a href="index.html"><img src="images/CORSMAL_logo.png" style="padding:0px 0px 0px 0px;height:75px" alt="CORSMAL"/></a>
						</p>
					</li>
				</ul>
			</div>
</div>
</div>
<!-- 2###################################################################################################### -->
<div class="wrapper row2"; style="margin-bottom: 7px;">
	<div class="rnd">
		<!-- ###### -->
		<div id="topnav">
			<ul style="margin-top: 5px;">
				<li><a href="index.html"><b>Home</b></a></li>
				<li><a href="objectives.html"><b>Objectives</b></a></li>
				<li><a href="publications.html"><b>Publications</b></a></li>
				<li><a href="blog.html"><b>Blog</b></a></li>
				<li><a href="events.html"><b>Events</b></a></li>
				<li><a href="code.html"><b>Code</b></a></li>
				<li class="active"><a href="data.html"><b>Data</b></a></li>
				<li><a href="benchmark.html"><b>Benchmark</b></a></li>
				<li><a href="challenge.html"><b>Challenge</b></a></li>
				<li class="last"><a href="team.html"><b>Team</b></a></li>
			</ul>
		</div>
		<!-- ###### -->
	</div>
</div>

<!-- 3####################################################################################################### -->
<div class="wrapper row1">
	<div id="container" class="clear">
		<div id="latestnewspage">
			<h2><p class=xmsonormal style='text-align:justify;text-justify:inter-ideograph'><b><span
				style='font-size:16.0pt'>Data</span></b><o:p></o:p></p></h2>

				<table class="tg" width="100%">
					<tr>
						<td class="tg-zv4m" style="vertical-align: top;">
							<a href="https://doi.org/10.17636/101CORSMAL1">
								<video width="200" autoplay loop><source  src="resources/output_4.mp4" type="video/mp4"> Your browser does not support HTML5 video. </video>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>CORSMAL Containers Manipulation</b><br>
								1,140 audio-visual-inertial recordings of people interacting with containers (e.g. pouring a liquid in a cup; shaking a food box). 15 containers;  3 filling levels; 3 types of filling. RGB, depth, and infrared images from 4 views; multi-channel audio from an  8-element circular microphone array.
								<br>
								[<a href="containers_manip.html"><u>details</u></a>]
								[<a href="https://doi.org/10.17636/101CORSMAL1"><u>data</u></a>]
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="https://doi.org/10.5281/zenodo.6372437" TARGET = "_blank">
									<img src="images/epfl_audio-tactile_dataset.png" alt="EPFL Audio-Tactile"  width="200" style="vertical-align: middle;"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Audio and Tactile dataset of robot object manipulation with different material contents</b><br>
								Auditory and tactile signals of a Kuka IIWA robot with an Allegro hand holding a plastic container containing different materials. The robot manipulates the container with vertical shaking and rotation motions. The data consists of force/pressure measurements on the Allegro hand using a Tekscan tactile skin sensor, auditory signals from a microphone, and the joints data of the IIWA robot and the Allegro hand joints.
								<br>
								[<a href="https://doi.org/10.5281/zenodo.6372437" TARGET = "_blank"><u>details</u></a>]
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="https://doi.org/10.17636/corsmal2 ">
									<img src="images/ICASSP20_dataset/000003.png" alt="Containers"  width="200" style="vertical-align: middle;"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>CORSMAL Containers</b><br>
								1,656 images of 23 containers (cups, drinking glasses, bottles) seen by two cameras (RGB, depth, and narrow-baseline stereo infrared) under different lighting and background conditions.
								<br>
								[<a href="containers.html"><u>details</u></a>]
								[<a href="https://doi.org/10.17636/corsmal2"><u>data</u></a>]
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="https://doi.org/10.5281/zenodo.4642577">
									<img src="images/RTL/ccm_dataset.png" alt="Containers"  width="200" style="vertical-align: middle;"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Crop - CORSMAL Containers Manipulation (C-CCM)</b><br>
								10,216 RGB images  automatically sampled from the three fixed views of the public videos recordings of the CORSMAL Container Manipulation dataset, and capturing cups (4) and drinking glasses (4) as containers under different lighting and background conditions. Containers are completely visible or occluded by the person's hand.
								<br>
								[<a href="https://doi.org/10.5281/zenodo.4642577"><u>details</u></a>]
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="https://doi.org/10.5281/zenodo.4770439" TARGET = "_blank">
									<img src="images/acc_image.png" alt="EUSIPCO21" width="200"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Audio-based Containers Manipulation Setup 2 (ACM-S2)</b><br>
								21 audio recordings  acquired  in  a different setup for  the  validation  of  audio-based  models  for  the  task  of  filling  type  and  filling  level classification.
								<br>
								[<a href="https://doi.org/10.5281/zenodo.4770439"><u>details</u></a>]
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="" TARGET = "_blank">
									<img src="https://www.epfl.ch/labs/lasa/wp-content/uploads/2021/02/Capture-dcran-2021-02-03-13.46.20.jpg" alt="Containers"  width="200" style="vertical-align: middle;"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Human-to-human handovers of objects with unknown content</b><br>
								219 configurations with synchronised video, poses (joints) and force sensors of 6 people manipulating and handing over 16 objects (4 drinking cups, 1 drinking glass, 1 mug, 1 food box, 1 pitcher and 8 common household objects) between each other in pairs. 
								<!-- Containers are empty, or filled at 50% or 90% with respect to their total capacity. -->
								<!-- A dataset that captures two subjects and 16 objects (eight container types and eight common household objects) 219 configurations executed by 6 participants, paired. Subject 1 grasps a container (or a common household object) and hands it over to Subject 2 (in the case of containers) or manipulates it and then hands it over (in the case of household objects) to Subject 2, who receives the object and places it back on the table to complete the task. -->
								<!-- Containers (four drinking cups, one drinking glass, one drinking mug, one food box, one pitcher) are made of different materials (plastic, glass, cardboard).  -->
								<!-- Containers are empty, or filled at 50% or 90% with respect to their total capacity. When filled, the food box contains pasta, whereas other containers are filled with water.  -->
								<!-- The dataset includes video data, pose (joints) and force sensors. All data are synchronised.  -->
								Dataset collected jointly with the SECONDHANDS EU H2020 project and in collaboration with Karlsruhe Institute of Technology (team of Prof. Tamim Asfour).
								<br>
								<!-- [<a href="https://www.epfl.ch/labs/lasa/datasets/" TARGET = "_blank"><u>details</u></a>] -->
							</td>
						</tr>
						<!--  -->
						<tr>
							<td class="tg-zv4m" style="vertical-align: top;">
								<a href="https://doi.org/10.5281/zenodo.5085800">
									<img src="images/pose/final_v4.png" alt="CHOC"  width="200" style="vertical-align: middle;"/>
								</a>
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>CORSMAL Hand-Occluded Containers (CHOC)</b><br>
								An image-based dataset for category-level 6D object pose and size estimation with 138,240 pseudo-realistic composite RGB-D images of hand-held containers on top of 30 real backgrounds (mixed-reality set) and 3,951 RGB-D images selected from the CORSMAL Container Manipulation dataset. 
								<br>
								[<a href="https://doi.org/10.5281/zenodo.5085800" TARGET = "_blank"><u>details</u></a>]
								<!-- [<a href="https://github.com/CORSMAL/CHOC-dataset-toolkit" TARGET = "_blank"><u>toolkit</u></a>] -->
							</td>
						</tr>
						<!--  -->
					</table>

					<br>

					<p>
						<b>Models</b>
					</p>

					<table class="tg" width="100%">
					<tr>
						<td class="tg-zv4m" style="vertical-align: top;" width="200">
						<!-- 	<a href="https://doi.org/10.5281/zenodo.4518951">
								<video width="200" autoplay loop><source  src="resources/output_4.mp4" type="video/mp4"> Your browser does not support HTML5 video. </video>
								</a> -->
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Filling level classification (image based)</b><br>
								Pre-trained models in PyTorch of the neural networks used in the paper <a href="filling.html"><u>Improving filling level classification with adversarial training</u></a>.
								<br>
								<a href="https://doi.org/10.5281/zenodo.4518951"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.4518951.svg" alt="DOI"></a>
							</td>
						</tr>
						<!--  -->
						<td class="tg-zv4m" style="vertical-align: top;" width="200">
						<!-- 	<a href="https://doi.org/10.5281/zenodo.4518951">
								<video width="200" autoplay loop><source  src="resources/output_4.mp4" type="video/mp4"> Your browser does not support HTML5 video. </video>
								</a> -->
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Real-to-simulation handovers</b><br>
								Pre-trained  models  and  3D  hand keypoints  annotations  to  be  used  with  the  implementation  of  the  real-to-simulation  framework  of  the paper <a href="safe_handover.html"><u>Towards safe human-to-robot handovers of unknown containers</u></a>.
								<br>
								<a href="https://doi.org/10.5281/zenodo.5525333"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.5525333.svg" alt="DOI"></a>
							</td>
						</tr>
						<!--  -->
						<td class="tg-zv4m" style="vertical-align: top;" width="200">
						<!-- 	<a href="https://doi.org/10.5281/zenodo.4518951">
								<video width="200" autoplay loop><source  src="resources/output_4.mp4" type="video/mp4"> Your browser does not support HTML5 video. </video>
								</a> -->
							</td>
							<td class="tg-zv4m" style="vertical-align: top;">
								<b>Audio classification</b><br>
								Neural  network's  architecture and  pre-trained  weights  used  in  the  paper <a href="https://arxiv.org/pdf/2103.15999.pdf"><u>Audio  Classification  of  the  Content  of  Food  Containers  and Drinking Glasses</u></a>, and the pre-trained models of the methods under comparison.
								<br>
								<a href="https://doi.org/10.5281/zenodo.4770061"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.4770061.svg" alt="DOI"></a>
								<!-- [<a href="audio_classification.html"><u>paper</u></a>] -->
							</td>
						</tr>
					<!--  -->
						<tr>
						<td class="tg-zv4m" style="vertical-align: top;">
<!-- 							<a href="https://github.com/amodas/PRIME-augmentations" TARGET = "_blank">
								<img src="images/prime_eccv22.png" alt="PRIME" width="200"/>
							</a> -->
						</td>
						<td class="tg-zv4m" style="vertical-align: top;">
							<b>PRIME: A few primitives can boost robustness to common corruptions</b><br>
							Models of the neural networks used in the paper <a href="https://doi.org/10.48550/arXiv.2112.13547"><u>PRIME: A few primitives can boost robustness to common corruptions</u></a> and pre-trained on CIFAR-10, CIFAR-100, ImageNet-100 and ImageNet using PRIME. Included also a model pre-trained on ImageNet-100 by combining DeepAugment + PRIME. The networks are implemented in PyTorch. <br>
							<a href="https://doi.org/10.5281/zenodo.5801871"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.5801871.svg" alt="DOI"></a>
						</td>
					</tr>
					<!--  -->
						<tr>
						<td class="tg-zv4m" style="vertical-align: top;">
<!-- 							<a href="https://github.com/amodas/PRIME-augmentations" TARGET = "_blank">
								<img src="images/prime_eccv22.png" alt="PRIME" width="200"/>
							</a> -->
						</td>
						<td class="tg-zv4m" style="vertical-align: top;">
							<b>CHOC-NOCS</b><br>
							Vision model based on a multi-branch convolutional neural network re-trained in QMUL for the task of category-level 6D pose estimation on real hand-occluded containers, and used in the pre-print <a href="http://arxiv.org/abs/2211.10470"><u>A mixed-reality dataset for category-level 6D pose and size estimation of hand-occluded containers</u></a>. The model is implemented in TensorFlow.<br>
							<a href="https://doi.org/10.5281/zenodo.7438645"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.7438645.svg" alt="DOI"></a>
						</td>
					</tr>
					</table>

 						<br>

					<p>
						<b>Other datasets of interest</b>	
					</p>	

					<li class="clear">
						<a href="https://rgbd-dataset.cs.washington.edu/" TARGET = "_blank">
							<div class="imglnews" style="border:0"><img src="images/rgbd_dataset.png" alt="RGB-D object dataset"  height="150" width="200"/></div>			  
						</a>
						<div class="latestnewspage">
							<p><b>RGB-D object dataset</b></p>			
							The RGB-D Object Dataset is a large dataset of 300 common household objects, recorded using a Kinect style 3D camera.<br>
							[<a href="https://rgbd-dataset.cs.washington.edu/" TARGET="_blank"><u>details</u></a>]
						</p>
					</div>
				</li>

				<li class="clear">
					<a href="https://cirl.lcsr.jhu.edu/research/human-machine-collaborative-systems/visual-perception/jhu-visual-perception-datasets/" TARGET = "_blank">
						<div class="imglnews" style="border:0"><img src="images/JHUVPD_dataset.png" alt="JHU Visual Perception Datasets"  height="150" width="200"/></div>			  
					</a>
					<div class="latestnewspage">
						<p><b>JHU Visual Perception Datasets (JHU-VPD)</b></p>		
						The JHU Visual Perception Datasets (JHU-VP) contain benchmarks for object recognition, detection and pose estimation using RGB-D data.
						<br>
						[<a href="https://cirl.lcsr.jhu.edu/research/human-machine-collaborative-systems/visual-perception/jhu-visual-perception-datasets/" TARGET = "_blank"><u>details</u></a>]
					</p>
				</div>
			</li>

			<li class="clear">
				<a href="http://rll.berkeley.edu/bigbird/" TARGET = "_blank">
					<div class="imglnews" style="border:0"><img src="images/BigBIRD.jpg" alt="BigBIRD: (Big) Berkeley instance recognition dataset"  height="150" width="200"/></div>			  
				</a>
				<div class="latestnewspage">
					<p><b>BigBIRD: (Big) Berkeley instance recognition dataset</b></p>			
					This is the dataset introduced with the following publication: A. Singh, J. Sha, K. Narayan, T. Achim, P. Abbeel, "A large-scale 3D database of object instances", Hong Kong, China, 31 May - 7 June 2014.
					<br>
					[<a href="http://rll.berkeley.edu/bigbird/" TARGET = "_blank"><u>details</u></a>]
				</p>
			</div>
		</li>

		<li class="clear">
			<a href="https://robotology.github.io/iCubWorld/#icubworld-transformations-modal" TARGET = "_blank">
				<div class="imglnews" style="border:0"><img src="images/icubworld.png" alt="iCubWorld Transformations"  height="150" width="200"/></div>			  
			</a>
			<div class="latestnewspage">
				<p><b>iCubWorld transformations</b></p>		
				In this dataset, each object is acquired while undergoing isolated visual transformations, in order to study invariance to real-world nuisances.
				<br>
				[<a href="https://robotology.github.io/iCubWorld/#icubworld-transformations-modal" TARGET = "_blank"><u>details</u></a>]
			</p>
		</div>
	</li>

	<li class="clear">
		<a href="http://cs.unc.edu/~ammirato/active_vision_dataset_website/" TARGET = "_blank">
			<div class="imglnews" style="border:0"><img src="images/AVD.jpg" alt="Active Vision Dataset (AVD)"  height="150" width="200"/></div>			  
		</a>
		<div class="latestnewspage">
			<p><b>Active Vision Dataset (AVD)</b></p>
			The dataset enables the simulation of motion for object instance recognition in real-world environments.
			<br>
			[<a href="http://cs.unc.edu/~ammirato/active_vision_dataset_website/" TARGET = "_blank"><u>details</u></a>]
		</p>
	</div>
</li>

<li class="clear">
	<a href="http://www-personal.umich.edu/~ywchao/hico/" TARGET = "_blank">
		<div class="imglnews" style="border:0"><img src="images/HICO.png" alt="HICO and HICO-DET"  height="150" width="200"/></div>			  
	</a>
	<div class="latestnewspage">
		<p><b>HICO and HICO-DET datasets</b></p>	
		Two benchmarks for classifying and detecting human-object interactions (HOI) in images: (i) HICO (Humans Interacting with Common Objects) and (ii) HICO-DET dataset.
		<br>
		[<a href="http://www-personal.umich.edu/~ywchao/hico/" TARGET = "_blank"><u>details</u></a>]
	</p>
</div>
</li>

<li class="clear">
	<a href="http://cocodataset.org" TARGET = "_blank">
		<div class="imglnews" style="border:0"><img src="images/COCO.jpg" alt="COCO dataset"  height="150" width="200"/></div>			  
	</a>
	<div class="latestnewspage">
		<p><b>COCO dataset</b></p>
		A large-scale object detection, segmentation, and captioning dataset with several different features.
		<br>
		[<a href="http://cocodataset.org" TARGET = "_blank"><u>details</u></a>]
	</p>
</div>
</li>
<li class="clear">
	<a href="https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/autonomous-robot-indoor-dataset/" TARGET = "_blank">
		<div class="imglnews" style="border:0"><img src="images/ARID.png" alt="Autonomous robot indoor dataset"  height="150" width="200"/></div>			  
	</a>
	<div class="latestnewspage">
		<p><b>Autonomous robot indoor dataset</b></p>			
		The dataset embeds the challenges faced by a robot in a real-life application and provides a useful tool for validating object recognition algorithms.
		<br>
		[<a href="https://www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/autonomous-robot-indoor-dataset/" TARGET = "_blank"><u>details</u></a>]
	</p>
</div>
</li>

<br><br>

</div>				
</div>
</div>



<div class="wrapper row1">
	<div id="header" class="clear" style="padding: 0px 0px 0px 20px;width:920px">
		<div class="fl_left" style="margin-top: 0px;">
			<ul>
				<li style="padding: 0 0 0 0">
					<p>
						<b>Sponsors</b>
						<br>
						<br>
					</p>
					<p>
						<a href="http://www.chistera.eu" TARGET = "_blank"><img src="images/chist-era_logo_crop.png" style="height:35px;" alt="Chistera logo"/></a>
						<a href="https://epsrc.ukri.org/" target="_blank"><img src="images/EPSRC_logo.png" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="EPSRC logo"></a>
						<a href="http://www.agence-nationale-recherche.fr/en/" target="_blank"><img src="images/ANR_logo.png" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="ANR logo"></a>
						<a href="http://www.snf.ch/en/Pages/default.aspx" target="_blank"><img src="images/FNS_logo.jpg" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="FNS logo"></a>
					</p>
				</li>
			</ul>
		</div>
		<div class="fl_right" style="margin-top: 0px;">
			<ul style="margin-bottom: 0px;">
				<li style="margin: 0px 4px 0px 0; padding: 0 0px 0 0;">
					<p>
						<b>Partners</b>
						<br>
						<br>
					</p>
					<p>
						<a href="https://www.qmul.ac.uk/" target="_blank"><img src="images/QMUL_logo.jpg" alt="Queen Mary University of London" style="padding:5px 10px 0px 0px;height:45px;"></a>
						<a href="http://www.sorbonne-universite.fr/en" target="_blank">
							<img src="images/Sorbonne_University_logo.png" alt="Sorbonne University" style="padding:5px 10px 0px 0px;height:45px;">
						</a>
						<a href="https://www.epfl.ch/en/home/" target="_blank">
							<img src="images/EPFL_logo.png" alt="EPFL" style="padding:5px 0px 0px 0px;height:45px;">
						</a>
					</p>
				</li>
			</ul>
		</div>
	</div>
</div>

<br><br>


<!-- ####################################################################################################### -->
<!-- <div class="clearing">&nbsp;</div>-->
<div class="wrapper bottomPg">
	<div id="copyright">
		<div id="fl_left" style="font-size:12px">
			Â© Copyright CORSMAL 2019-2022
		</div>	  
	</div>
</div>
<!-- footer -->
<!--added to clear error with content element -->
</body>
</html>
