<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="EN" lang="EN" dir="ltr">
<head profile="http://gmpg.org/xfn/11"> 
	<link rel="shortcut icon" href="favicon.ico" /> 

<title>
	CORSMAL: Collaborative object recognition, shared manipulation and learning | The CORSMAL Challenge
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
<meta http-equiv="imagetoolbar" content="no" />
<meta http-equiv="KeyWords" content="code, software, CORSMAL, robotics, touch, vision, audio, signal processing, human behaviour, competition, challenge, ICPR"/>
<meta name="image" property="og:image" content="images/CORSMAL_logo.png">
<link rel="stylesheet" href="css/layout.css" type="text/css" /> 
<script type="text/javascript" src="js/jquery-1.4.1.min.js"></script>  
<script type="text/javascript" src="js/jquery.slidepanel.setup.js"></script> 
<script type="text/javascript" src="js/jquery-ui-1.7.2.custom.min.js"></script>
<script type="text/javascript" src="js/jquery.tabs.setup.js"></script>

<script type="text/javascript">
	window.TableLoader = {
		tables:{},
		register:function(table, func){
			this.tables[table] = func;
		},
		trigger:function(key){
			var self = this;

			if(this.tables[key]){
				this.tables[key]();
				this.tables[key] = function(){};

				var keys = Object.keys(this.tables);
				var index = keys.indexOf(key);

				if(index){
					this.tables[keys[index - 1]]();
					this.tables[keys[index - 1]] = function(){};
				}

				if(index < keys.length - 1){
					this.tables[keys[index + 1]]();
					this.tables[keys[index + 1]] = function(){};
				}
			}

			if(key == "theming"){
				var themes = Object.keys(this.tables).slice(-7);

				themes.forEach(function(item){
					self.trigger(item);
				})
			}
		},
		loadFirst:function(){
			first = Object.keys(this.tables)[0];

			if(first){
				this.trigger(first);
			}
		}
	}
</script>

<script type="text/javascript">
	var tabledata =
	[
	{id:1,team:"Random",modality:"",view1:null,view2:null,view3:null,view4:null,task1:33.35.toFixed(2),task2:21.24.toFixed(2),task3:31.63.toFixed(2),global:38.47.toFixed(2)},
	{id:2,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:true,view2:null,view3:null,view4:null,task1:58.51.toFixed(2),task2:30.85.toFixed(2),task3:null,global:19.46.toFixed(2)},	
	{id:3,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:true,view3:null,view4:null,task1:48.90.toFixed(2),task2:28.75.toFixed(2),task3:null,global:17.28.toFixed(2)},
	{id:4,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:true,view4:null,task1:36.52.toFixed(2),task2:21.14.toFixed(2),task3:null,global:15.15.toFixed(2)},
	{id:5,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:null,view4:true,task1:25.12.toFixed(2),task2:14.12.toFixed(2),task3:null,global:12.95.toFixed(2)},
	{id:6,team:"NTNU-ERC",modality:"Audio+Depth",view1:null,view2:null,view3:true,view4:null,task1:null,task2:81.97.toFixed(2),task3:66.92.toFixed(2),global:38.56.toFixed(2)},
	{id:7,team:"Challengers",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:50.73.toFixed(2),task2:78.58.toFixed(2),task3:null,global:29.25.toFixed(2)},
	// {id:8,team:"Because It's Tactile",modality:"RGB+D+IR+Audio",view1:true,view2:true,view3:null,view4:null,task1:79.38.toFixed(2),task2:93.83.toFixed(2),task3:37.95.toFixed(2),global:42.05.toFixed(2)},
	{id:8,team:"Because It's Tactile",modality:"RGB+D+IR+Audio",view1:true,view2:true,view3:null,view4:null,task1:78.14.toFixed(2),task2:93.83.toFixed(2),task3:60.56.toFixed(2),global:64.98.toFixed(2)},
	{id:9,team:"HVRL",modality:"Audio+RGB+D",view1:true,view2:null,view3:null,view4:null,task1:82.63.toFixed(2),task2:97.83.toFixed(2),task3:57.19.toFixed(2),global:63.32.toFixed(2),runtime:467},
	{id:10,team:"Concatenation",modality:"",view1:null,view2:null,view3:null,view4:null,task1:44.31.toFixed(2),task2:41.77.toFixed(2),task3:63.00.toFixed(2),global:52.80.toFixed(2)},
	{id:11,team:"SCC-Net",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:84.21.toFixed(2),task2:93.34.toFixed(2),task3:null,global:28.02.toFixed(2)},
	// {id:2,team:"Smartcameras",modality:"RGB videos",view1:true,view2:null,view3:true,view4:true,task1:0.00,task2:0.00,task3:30.69,global:18.25},
	];

	var tabledatapriv =
	[
	{id:1,team:"Random",modality:"",view1:null,view2:null,view3:null,view4:null,task1:41.86.toFixed(2),task2:27.52.toFixed(2),task3:17.53.toFixed(2),global:31.65.toFixed(2)},
	{id:2,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:true,view2:null,view3:null,view4:null,task1:32.93.toFixed(2),task2:13.04.toFixed(2),task3:null,global:9.59.toFixed(2)},	
	{id:3,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:true,view3:null,view4:null,task1:26.73.toFixed(2),task2:15.54.toFixed(2),task3:null,global:6.99.toFixed(2)},
	{id:4,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:true,view4:null,task1:25.52,task2:9.04.toFixed(2),task3:null,global:9.96.toFixed(2)},
	{id:5,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:null,view4:true,task1:21.99,task2:11.23.toFixed(2),task3:null,global:10.25.toFixed(2)},
	{id:6,team:"NTNU-ERC",modality:"Audio+Depth",view1:null,view2:null,view3:true,view4:null,task1:null,task2:91.67.toFixed(2),task3:67.67.toFixed(2),global:39.80.toFixed(2)},
	{id:7,team:"Challengers",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:47.08.toFixed(2),task2:71.75.toFixed(2),task3:null,global:23.21.toFixed(2)},
	{id:8,team:"Because It's Tactile",modality:"RGB+D+IR+Audio",view1:true,view2:true,view3:null,view4:null,task1:81.16.toFixed(2),task2:94.70.toFixed(2),task3:60.58.toFixed(2),global:65.15.toFixed(2)},
	{id:9,team:"HVRL",modality:"Audio+RGB+D",view1:true,view2:null,view3:null,view4:null,task1:74.43.toFixed(2),task2:96.08.toFixed(2),task3:52.38.toFixed(2),global:61.01.toFixed(2)},
	{id:10,team:"Concatenation",modality:"",view1:null,view2:null,view3:null,view4:null,task1:42.70.toFixed(2),task2:41.90.toFixed(2),task3:62.14.toFixed(2),global:54.14.toFixed(2)},
	{id:11,team:"SCC-Net",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:80.98,task2:92.58.toFixed(2),task3:null,global:22.92.toFixed(2)},
	// {id:1,team:"Baseline",modality:"RGB images",view1:null,view2:null,view3:true,view4:true,task1:35.48,task2:26.42,task3:80.58,global:52.78},
	// {id:2,team:"Smartcameras",modality:"RGB videos",view1:true,view2:null,view3:true,view4:true,task1:0.00,task2:0.00,task3:30.69,global:18.25},
	];


	var tabledatacomb =
	[
	{id:1,team:"Random",modality:"",view1:null,view2:null,view3:null,view4:null,task1:37.62.toFixed(2),task2:24.38.toFixed(2),task3:24.58.toFixed(2),global:35.06.toFixed(2)},
	{id:2,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:true,view2:null,view3:null,view4:null,task1:47.00.toFixed(2),task2:23.05.toFixed(2),task3:null,global:14.53.toFixed(2)},	
	{id:3,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:true,view3:null,view4:null,task1:39.00.toFixed(2),task2:22.90.toFixed(2),task3:null,global:12.14.toFixed(2)},
	{id:4,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:true,view4:null,task1:31.46.toFixed(2),task2:15.63.toFixed(2),task3:null,global:12.56.toFixed(2)},
	{id:5,team:"Mask R-CNN + ResNet-18",modality:"RGB",view1:null,view2:null,view3:null,view4:true,task1:23.68.toFixed(2),task2:12.70.toFixed(2),task3:null,global:11.60.toFixed(2)},
	{id:6,team:"NTNU-ERC",modality:"Audio+Depth",view1:null,view2:null,view3:true,view4:null,task1:null,task2:86.89.toFixed(2),task3:67.30.toFixed(2),global:39.18.toFixed(2)},
	{id:7,team:"Challengers",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:48.71.toFixed(2),task2:75.24.toFixed(2),task3:null,global:26.23.toFixed(2)},
	{id:8,team:"Because It's Tactile",modality:"RGB+D+IR+Audio",view1:true,view2:true,view3:null,view4:null,task1:79.65.toFixed(2),task2:94.26.toFixed(2),task3:60.57.toFixed(2),global:65.06.toFixed(2)},
	{id:9,team:"HVRL",modality:"Audio+RGB+D",view1:true,view2:null,view3:null,view4:null,task1:78.56.toFixed(2),task2:96.95.toFixed(2),task3:54.79.toFixed(2),global:62.16.toFixed(2)},
	{id:10,team:"Concatenation",modality:"",view1:null,view2:null,view3:null,view4:null,task1:43.53.toFixed(2),task2:41.83.toFixed(2),task3:62.57.toFixed(2),global:53.47.toFixed(2)},
	{id:11,team:"SCC-Net",modality:"Audio",view1:null,view2:null,view3:null,view4:null,task1:82.66.toFixed(2),task2:93.09.toFixed(2),task3:null,global:25.47.toFixed(2)},
	// {id:1,team:"Baseline",modality:"RGB images",view1:null,view2:null,view3:true,view4:true,task1:35.48,task2:26.42,task3:80.58,global:52.78},
	// {id:2,team:"Smartcameras",modality:"RGB videos",view1:true,view2:null,view3:true,view4:true,task1:0.00,task2:0.00,task3:30.69,global:18.25},
	];

	var tabledatatask1 =
	[
	{id:1,team:"Random",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:33.35.toFixed(2),private:41.86.toFixed(2),combined:37.62.toFixed(2)},
	{id:2,team:"Mask R-CNN + RN18",audio:null,rgb1:true,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:58.51.toFixed(2),private:32.93.toFixed(2),combined:47.00.toFixed(2)},
	{id:3,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:48.90.toFixed(2),private:26.73.toFixed(2),combined:39.00.toFixed(2)},
	{id:4,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:36.52.toFixed(2),private:25.52.toFixed(2),combined:31.46.toFixed(2)},
	{id:5,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:25.12.toFixed(2),private:21.99.toFixed(2),combined:23.68.toFixed(2)},
	{id:6,team:"Challengers",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:50.73.toFixed(2),private:47.08.toFixed(2),combined:48.71.toFixed(2)},
	// {id:7,team:"NTNU-ERC",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:33.35.toFixed(2),private:41.86.toFixed(2),combined:37.62.toFixed(2)},
	{id:8,team:"Concatenation",audio:true,rgb1:true,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:44.31.toFixed(2),private:42.70.toFixed(2),combined:43.53.toFixed(2)},
	{id:9,team:"HVRL",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:82.63.toFixed(2),private:74.43.toFixed(2),combined:78.56.toFixed(2)},
	{id:10,team:"Because It's Tactile",audio:true,rgb1:true,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:78.14.toFixed(2),private:81.16.toFixed(2),combined:79.65.toFixed(2)},
	// {id:11,team:"SCC-Net",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:84.21.toFixed(2),private:80.98.toFixed(2),combined:82.66.toFixed(2)},
	{id:11,team:"ACC",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:80.22.toFixed(2),private:81.46.toFixed(2),combined:80.84.toFixed(2)},
	{id:12,team:"ZCR+MFCC+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:63.63.toFixed(2),private:54.97.toFixed(2),combined:59.35.toFixed(2)},
	{id:13,team:"ZCR+MFCC+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:66.27.toFixed(2),private:57.19.toFixed(2),combined:61.87.toFixed(2)},
	{id:14,team:"ZCR+MFCC+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:70.04.toFixed(2),private:63.11.toFixed(2),combined:66.80.toFixed(2)},
	{id:15,team:"A5F+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:55.49.toFixed(2),private:53.22.toFixed(2),combined:54.47.toFixed(2)},
	{id:16,team:"A5F+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:60.77.toFixed(2),private:58.57.toFixed(2),combined:60.09.toFixed(2)},
	{id:17,team:"A5F+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:64.18.toFixed(2),private:63.94.toFixed(2),combined:64.74.toFixed(2)},
	{id:18,team:"Spectrogram+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:59.15.toFixed(2),private:53.47.toFixed(2),combined:56.38.toFixed(2)},
	{id:19,team:"Spectrogram+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:47.66.toFixed(2),private:51.54.toFixed(2),combined:49.67.toFixed(2)},
	{id:20,team:"Spectrogram+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:45.43.toFixed(2),private:45.59.toFixed(2),combined:45.49.toFixed(2)},
	{id:21,team:"Spectrogram+PCA+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:39.03.toFixed(2),private:37.16.toFixed(2),combined:38.31.toFixed(2)},
	{id:22,team:"Spectrogram+PCA+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:30.08.toFixed(2),private:31.99.toFixed(2),combined:31.64.toFixed(2)},
	{id:23,team:"Spectrogram+PCA+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:46.79.toFixed(2),private:42.46.toFixed(2),combined:44.66.toFixed(2)},
	];

	var tabledatatask2 =
	[
	{id:1,team:"Random",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:21.24.toFixed(2),private:27.52.toFixed(2),combined:24.38.toFixed(2)},
	{id:2,team:"Mask R-CNN + RN18",audio:null,rgb1:true,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:30.85.toFixed(2),private:13.04.toFixed(2),combined:23.05.toFixed(2)},
	{id:3,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:28.75.toFixed(2),private:15.54.toFixed(2),combined:22.90.toFixed(2)},
	{id:4,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:21.14.toFixed(2),private:9.04.toFixed(2),combined:15.63.toFixed(2)},
	{id:5,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:14.12.toFixed(2),private:11.23.toFixed(2),combined:12.70.toFixed(2)},
	{id:6,team:"Challengers",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:78.58.toFixed(2),private:71.75.toFixed(2),combined:75.24.toFixed(2)},
	{id:7,team:"NTNU-ERC",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:81.97.toFixed(2),private:91.67.toFixed(2),combined:86.89.toFixed(2)},
	{id:8,team:"Concatenation",audio:true,rgb1:true,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:41.77.toFixed(2),private:41.90.toFixed(2),combined:41.83.toFixed(2)},
	{id:9,team:"HVRL",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:97.83.toFixed(2),private:96.08.toFixed(2),combined:96.95.toFixed(2)},
	{id:10,team:"Because It's Tactile",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:93.83.toFixed(2),private:94.70.toFixed(2),combined:94.26.toFixed(2)},
	// {id:11,team:"SCC-Net",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:93.34.toFixed(2),private:92.85.toFixed(2),combined:93.09.toFixed(2)},
	{id:11,team:"ACC",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:95.12.toFixed(2),private:93.92.toFixed(2),combined:94.50.toFixed(2)},
	{id:12,team:"ZCR+MFCC+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:88.19.toFixed(2),private:79.23.toFixed(2),combined:83.73.toFixed(2)},
	{id:13,team:"ZCR+MFCC+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:84.23.toFixed(2),private:71.96.toFixed(2),combined:78.67.toFixed(2)},
	{id:14,team:"ZCR+MFCC+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:92.97.toFixed(2),private:89.74.toFixed(2),combined:91.31.toFixed(2)},
	{id:15,team:"A5F+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:76.84.toFixed(2),private:75.96.toFixed(2),combined:76.41.toFixed(2)},
	{id:16,team:"A5F+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:64.92.toFixed(2),private:79.72.toFixed(2),combined:72.86.toFixed(2)},
	{id:17,team:"A5F+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:88.36.toFixed(2),private:87.46.toFixed(2),combined:87.88.toFixed(2)},
	{id:18,team:"Spectrogram+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:60.50.toFixed(2),private:68.58.toFixed(2),combined:64.55.toFixed(2)},
	{id:19,team:"Spectrogram+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:39.39.toFixed(2),private:41.81.toFixed(2),combined:40.61.toFixed(2)},
	{id:20,team:"Spectrogram+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:47.98.toFixed(2),private:47.68.toFixed(2),combined:47.82.toFixed(2)},
	{id:21,team:"Spectrogram+PCA+kNN",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:24.47.toFixed(2),private:28.34.toFixed(2),combined:26.53.toFixed(2)},
	{id:22,team:"Spectrogram+PCA+SVM",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:20.57.toFixed(2),private:27.60.toFixed(2),combined:24.20.toFixed(2)},
	{id:23,team:"Spectrogram+PCA+RF",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:28.75.toFixed(2),private:37.79.toFixed(2),combined:33.32.toFixed(2)},
	];

	var tabledatatask3 =
	[
	{id:1,team:"Random",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:31.63.toFixed(2),private:17.53.toFixed(2),combined:24.58.toFixed(2)},
	// {id:2,team:"Mask R-CNN + RN18",audio:null,rgb1:true,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:30.85.toFixed(2),private:13.04.toFixed(2),combined:23.05.toFixed(2)},
	// {id:3,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:28.75.toFixed(2),private:15.54.toFixed(2),combined:22.90.toFixed(2)},
	// {id:4,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:21.14.toFixed(2),private:9.04.toFixed(2),combined:15.63.toFixed(2)},
	// {id:5,team:"Mask R-CNN + RN18",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:14.12.toFixed(2),private:11.23.toFixed(2),combined:12.70.toFixed(2)},
	// {id:6,team:"Challengers",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:78.58.toFixed(2),private:71.75.toFixed(2),combined:75.24.toFixed(2)},
	{id:7,team:"NTNU-ERC",audio:null,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:true,ir3:null,rgb4:null,depth4:null,ir4:null,public:66.92.toFixed(2),private:67.67.toFixed(2),combined:67.30.toFixed(2)},
	{id:8,team:"Concatenation",audio:null,rgb1:true,depth1:null,ir1:null,rgb2:true,depth2:null,ir2:null,rgb3:true,depth3:null,ir3:null,rgb4:true,depth4:null,ir4:null,public:63.00.toFixed(2),private:62.14.toFixed(2),combined:62.57.toFixed(2)},
	{id:9,team:"HVRL",audio:null,rgb1:true,depth1:true,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:57.19.toFixed(2),private:52.38.toFixed(2),combined:54.79.toFixed(2)},
	{id:10,team:"Because It's Tactile",audio:null,rgb1:true,depth1:true,ir1:true,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:60.56.toFixed(2),private:60.58.toFixed(2),combined:60.57.toFixed(2)},
	// {id:11,team:"SCC-Net",audio:true,rgb1:null,depth1:null,ir1:null,rgb2:null,depth2:null,ir2:null,rgb3:null,depth3:null,ir3:null,rgb4:null,depth4:null,ir4:null,public:93.34.toFixed(2),private:92.85.toFixed(2),combined:93.09.toFixed(2)},
	];

	var tabledatatask4 =
	[
	{id:1,team:"Random",desc:"Baseline with random estimations for each task.",task1:true,task2:true,task3:true,public:38.47.toFixed(2),private:31.65.toFixed(2),combined:35.06.toFixed(2)},
	{id:2,team:"Mask R-CNN + RN18",desc:"Vision baseline for filling properties estimation.",task1:true,task2:true,task3:null,public:19.46.toFixed(2),private:9.59.toFixed(2),combined:14.53.toFixed(2)},
	{id:3,team:"Mask R-CNN + RN18",desc:"Vision baseline for filling properties estimation.",task1:true,task2:true,task3:null,public:17.28.toFixed(2),private:6.99.toFixed(2),combined:12.14.toFixed(2)},
	{id:4,team:"Mask R-CNN + RN18",desc:"Vision baseline for filling properties estimation.",task1:true,task2:true,task3:null,public:15.15.toFixed(2),private:9.96.toFixed(2),combined:12.56.toFixed(2)},
	{id:5,team:"Mask R-CNN + RN18",desc:"Vision baseline for filling properties estimation.",task1:true,task2:true,task3:null,public:12.95.toFixed(2),private:10.25.toFixed(2),combined:11.60.toFixed(2)},
	{id:6,team:"Challengers",desc:"Sound-based classification of filling type and level with STFT and 5-layers fully connected neural network.",task1:true,task2:true,task3:null,public:29.25.toFixed(2),private:23.21.toFixed(2),combined:26.23.toFixed(2)},
	{id:7,team:"NTNU-ERC",desc:"MFCC features in a 20s-window + neural network to classify filling type. Object detection and selection of the closest contours (up to 700 mm) in the depth data + regression with a CNN for container capacity. ",task1:null,task2:true,task3:true,public:38.56.toFixed(2),private:39.80.toFixed(2),combined:39.18.toFixed(2)},
	{id:8,team:"Concatenation",desc:"Multi-modal learning with audio features and prior of container categories through object detection for inferring container capacity and fluid properties.",task1:true,task2:true,task3:true,public:52.80.toFixed(2),private:54.14.toFixed(2),combined:53.47.toFixed(2)},
	{id:9,team:"HVRL",desc:"Log-Mel spectrogram-based audio features as input to VGG-based CNN and LSTM for filling properties estimation. Container volume from the shape approximation as cuboid of the 3D point cloud obtained with RGB-D data and object detection with Mask R-CNN.",task1:true,task2:true,task3:true,public:63.32.toFixed(2),private:61.01.toFixed(2),combined:62.16.toFixed(2)},
	{id:10,team:"Because It's Tactile",desc:"GRU+ Random Forest for filling properties estimation. LoDE with RGB-D-IR data from selected frames in a video for volume estimation.",task1:true,task2:true,task3:true,public:64.98.toFixed(2),private:65.15.toFixed(2),combined:65.06.toFixed(2)},
	// {id:11,team:"SCC-Net",desc:"Sound-based hierarchical ensemble of DNNs to jointly classify filling type and level.",task1:true,task2:true,task3:null,public:28.02.toFixed(2),private:22.92.toFixed(2),combined:25.47.toFixed(2)},
	{id:11,team:"ACC",desc:"Sound-based model that first identifies the action performed by a person with a container and then determines the amount and type of content using an action-specific  classifier. The models consists of three independent CNN classifiers and combines content types and levels into a set of seven feasible classes.",task1:true,task2:true,task3:null,public:28.25.toFixed(2),private:21.89.toFixed(2),combined:25.07.toFixed(2)},
	];
</script>

<script> 
  function AppearMetadata(metadata_id) {
  	var y = document.getElementsByClassName("metadata");
  	for (var i = 0; i < y.length; i ++) {
    	y[i].style.display = 'none';
		}

    var x = document.getElementById(metadata_id);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
</script>


<link href="benchmark/dist/css/tabulator.min.css" rel="stylesheet">
<script type="text/javascript" src="benchmark/dist/js/tabulator.min.js"></script>
<link href="https://unpkg.com/tabulator-tables@4.4.3/dist/css/tabulator.min.css" rel="stylesheet">
<script type="text/javascript" src="https://unpkg.com/tabulator-tables@4.4.3/dist/js/tabulator.min.js"></script>

<style>
	/* NAVIGATION */
	nav {0
		width: 100%;
		margin: 0 auto;
		background: #fff;
	}

	/* By Dominik Biedebach @domobch */
	nav ul {
		list-style: none;
		text-align: center;
	}
	nav ul li {
		display: inline-block;
	}
	nav ul li a {
		display: block;
		padding: 15px;
		text-decoration: none;
		color: #aaa;
		font-weight: 800;
		margin: 0 10px;
	}
	nav ul li a,
	nav ul li a:after,
	nav ul li a:before {
		transition: all .5s;
	}
	nav ul li a:hover {
		color: #555;
	}

	/* stroke */
	nav.stroke ul li a,
	nav.fill ul li a {
		position: relative;
	}
	nav.stroke ul li a:after,
	nav.fill ul li a:after {
		position: absolute;
		bottom: 0;
		left: 0;
		right: 0;
		margin: auto;
		width: 0%;
		content: '.';
		color: transparent;
		background: #333;
		height: 1px;
	}
	nav.stroke ul li a:hover:after {
		width: 100%;
	}


	#teams {
	  list-style-type: none;
	  margin: 0;
	  padding: 0;
	  width: 200px;
	  background-color: #f1f1f1;
	}

	#teams li a {
	  display: block;
	  color: #000;
	  padding: 8px 16px;
	  text-decoration: none;
	}

	/* Change the link color on hover */
	#teams li a:hover {
	  background-color: #555;
	  color: white;
	}

</style>

</head>

<body id="top">
	<div class="wrapper row1">
		<div id="header" class="clear">
			<div class="fl_left">
				<ul>
					<li>
						<p>
							<a href="index.html"><img src="images/CORSMAL_logo.png" style="padding:0px 0px 0px 0px;height:75px" alt="CORSMAL"/></a>
						</p>
					</li>
				</ul>
			</div>
<!--<div class="fl_right"> <a href="index.html">  <li><h1 style="color: #fff; font-size:370%;">CIS</h1>  <h1 style="color: #fff; font-size: 160%;">Centre for Intelligent Sensing</h1>  </li> </a> 
</div>--> 
</div>
</div>
<!--2###################################################################################################### --> 
<div class="wrapper row2"; style="margin-bottom: 7px;">
	<div class="rnd"> <!-- ###### --> 
		<div id="topnav"> 
			<ul style="margin-top: 5px;">
				<li><a href="index.html"><b>Home</b></a></li> 
				<li><a href="objectives.html"><b>Objectives</b></a></li>
				<li><a href="publications.html"><b>Publications</b></a></li> 
				<li><a href="blog.html"><b>Blog</b></a></li> 
				<!-- <li class="active"><a href="events.html"><b>Events</b></a></li> -->
				<li><a href="events.html"><b>Events</b></a></li>
				<li ><a href="code.html"><b>Code</b></a></li>
				<li><a href="data.html"><b>Data</b></a></li> 
				<li><a href="benchmark.html"><b>Benchmark</b></a></li>
				<li class="active"><a href="challenge.html"><b>Challenge</b></a></li>
				<li class="last"><a href="team.html"><b>Team</b></a></li>
			</ul>
		</div>
		<!-- ###### --> 
	</div>
</div>
<!-- 3####################################################################################################### --> 
<div class="wrapper row1"> 
	<div id="container" class="clear">
		<div id="latestnewspage" class="clear">
			<h2>
				<p class="xmsonormal" style="text-align:left;text-justify:inter-ideograph;margin-bottom:0px">
					<b><span style="font-size:16.0pt">CORSMAL Challenge: Multi-modal fusion and learning for robotics</span></b>
				</p>
			</h2>
<!-- 			<h2>
				<p class="xmsonormal" style="text-align:center;text-justify:inter-ideograph;margin-bottom:5px">
					<b><span style="font-size:16.0pt"> </span></b><o:p></o:p>
				</p>
			</h2> -->

			<div>
				<p align="center">
				<video style="border: 3px solid #EEE;" width="500" controls>
					<source src="resources/ICPR2020.mp4" type="video/mp4">
					Your browser does not support the video tag.</video>
					<br><br>
					<!-- <a href="resources/ICPR2020_CORSMAL_Challenge_CFP.pdf" target="_blank"><u>Call for participation</u></a> -->
				</p>
				<p align="center">
					<!-- <b>The Challenge is ongoing and we accept new submissions.</b> -->
					<!-- <br> -->
					<!-- If you are interested and want to participate, please register with this <a href="https://forms.gle/wsDpvdGGEyGyAEHj6" target="_blank"><u>form</u></a>. -->
					<!-- <br> -->
					The ongoing challenge has been updated with additional tasks and performance scores.<br>Click <a href="challenge.html"><u>here</u></a> for more info and results.
					<!-- <br> -->
					<!-- Info and queries: <a href="mailto:corsmal-challenge@qmul.ac.uk"><u>corsmal-challenge@qmul.ac.uk</u></a> -->
					<br>
				</p>
			</div>
			<nav class="stroke" align="center">
					<ul>
						<!-- <li><a href="#description">Description</a></li> -->
						<li><a href="#tasks">Tasks</a></li>
						<li><a href="#dataset">Dataset</a></li>
						<li><a href="#documentation">Documentation</a></li>
						<li><a href="#leaderboard">Leaderboards</a></li>
						<li><a href="#evaluation">Evaluation</a></li>
						<!-- <li><a href="#publications">Publications</a></li> -->						
						<li><a href="#submission">Submission</a></li>
					</ul>
				</nav>

			<div>
				<p id="description">
					<b><span style="font-size:14.0pt">Description</span></b>
				</p>
				<p align="justify"> 
					A major challenge for human-robot cooperation in household chores is enabling robots to predict the properties of previously unseen containers with different fillings. Examples of containers are cups, glasses, mugs, bottles and food boxes, whose varying physical properties such as material, stiffness, texture, transparency, and shape must be inferred on-the-fly prior to a pick-up or a handover. 
				</p> 
				<p align="justify"> 
					The challenge focuses on the estimation of the capacity and mass of containers, as well as the type, mass and percentage of the content, if any. 
					Participants will determine these physical properties of a container while it is manipulated by a human. Containers vary in their physical properties (shape, material, texture, transparency, and deformability). Containers and fillings are not known to the robot: the only prior information available is a set of object categories (glasses, cups, food boxes) and a set of filling types (water, pasta, rice). Previous and related challenges/benchmarks, such as the <a href="http://amazonpickingchallenge.org" ,=""target="_blank"><u>Amazon Picking Challenge</u></a> or the <a href="http://www.ycbbenchmarks.com" ,=""target="_blank"><u>Yale-CMU-Berkeley (YCB) benchmark</u></a>, focus on tasks where robots interact with objects on a table and without the presence of a human, for example grasping objects, table setting, stacking cups, or assembling/disassembling objects.
				</p>
				<p align="justify">
					Advancements in this research field will help the integration of smart robots into people's lives to perform daily activities involving objects and handovers. For example, this is a step towards supporting people with disabilities in performing everyday activities.
				</p>	
			</div>

			<div>
				<p align="justify"> 
						<p id="dataset">
							<b><span style="font-size:14.0pt">The CORSMAL Containers Manipulation dataset</span></b>
						</p>
						<p align="justify">
					CORSMAL distributes an audio-visual-inertial dataset of people interacting with containers, for example while pouring a filling into a glass or shaking a food box. The dataset is collected with four multi-sensor devices (one on a robotic arm, one on the human chest and two third-person views) and a circular microphone array. Each device is equipped with an RGB camera, a stereo infrared camera and an inertial measurement unit. In addition to RGB and infrared images, each device provides synchronised depth images that are spatially aligned with the RGB images. All signals are synchronised, and the calibration information for all devices, as well as the inertial measurements of the body-worn device, is also provided.
				</p>
						<table align="center" style="border:0px;padding-left:0px"> 
							<tr>
								<td width="18%" style="border:0px;padding:0px" align="center">Camera 1</td>
								<td width="18%" style="border:0px;padding:0px" align="center">Camera 2</td>
								<td width="18%" style="border:0px;padding:0px" align="center">Camera 3</td>
								<td width="18%" style="border:0px;padding:0px" align="center">Camera 4</td>
								<td width="18%" style="border:0px;padding:0px" align="center">Audio</td>
							</tr>
							<tr>
								<td style="border:0px;padding:0px 0px 10px 0px;vertical-align: middle;" align="center"><img src="resources/wine_glass/s1_fi2_fu1_b1_l1_c1.gif" alt=”animated” width="150px" /></td>
								<td style="border:0px;padding:0px 0px 10px 0px;vertical-align: middle;" align="center"><img src="resources/wine_glass/s1_fi2_fu1_b1_l1_c2.gif" alt=”animated” width="150px" /></td>
								<td style="border:0px;padding:0px 0px 10px 0px;vertical-align: middle;" align="center"><img src="resources/wine_glass/s1_fi2_fu1_b1_l1_c3.gif" alt=”animated” width="150px" /></td>
								<td style="border:0px;padding:0px 0px 10px 0px;vertical-align: middle;" align="center"><img src="resources/wine_glass/s1_fi2_fu1_b1_l1_c4.gif" alt=”animated” width="150px" /></td>
								<td style="border:0px;padding:0px 0px 10px 0px;vertical-align: middle;" align="center"> 
									<audio style="width:100%" controls> <source src="resources/wine_glass/s1_fi2_fu1_b1_l1_audio.wav" type="audio/wav"> Your browser does not support the audio tag.
									</audio>
								</td>
							</tr>
						</table>
						<p align="justify">
							The dataset consists of 1140 audio-visual-inertial recordings of people interacting with (15) containers, using 4 cameras (RGB, depth, and infrared) and a 8-element circular microphone array. Containers are either empty (0%) or filled at 2 different levels (50%, 90%) with 3 different types of content (water, pasta, rice). For example, people can pour a liquid in a glass/cup or shake a food box.
						</p>
						<p align="justify">
							The dataset is split into training set (9 containers), public testing set (3 containers), and private testing set (3 containers). We provide to the participants with the annotations of the capacity of the container, filling type, filling level, the mass of the container, and the mass of the filling for the training set. No annotations will be provided for public testing set while private testing set will not be released to the participants. The containers for each set are evenly distributed among the three container types.
						</p>
						<p align="justify"> 
					The CORSMAL Challenge includes three scenarios with an increasing level of difficulty, caused by occlusions or subject motions: 
					<ul style="padding:0px">
						<li class="clear">			
							<p>
								<img src="images/s1.png" alt="Scenario 1" style="float:left; margin:0 15px 15px 0; clear:left; width:150px;"/>
								Scenario 1. The subject sits in front of the robot, while a container is on a table. The subject pours the filling into the container, while trying not to touch the container, or shakes an already filled food box, and then initiates the handover of the container to the robot.
							</p>
						</li>
						<li class="clear">			
							<p>
								<img src="images/s2.png" alt="Scenario 2" style="float:left; margin:0 15px 15px 0; clear:left; width:150px;"/>
								Scenario 2.
								The subject sits in front of the robot, while holding a container. The subject pours the filling from a jar into a glass/cup or shakes an already filled food box, and then initiates the handover of the container to the robot. 
							</p>
						</li>
						<li class="clear">			
							<p>
								<img src="images/s3.png" alt="Scenario 3" style="float:left; margin:0 15px 15px 0; clear:left; width:150px;"/>
								Scenario 3.
								A container is held by the subject while standing to the side of the robot, potentially visible from one third-person view camera only. The subject pours the filling from a jar into a glass/cup or shakes an already filled food box, takes a few steps to reach the front of the robot and then initiates the handover of the container to the robot. 
							</p>
						</li>
					</ul>
					Each scenario is recorded with two different backgrounds and under two different lighting conditions. <br>
				</p>
				<p><a href="containers_manip.html" target="_blank"><u>Webpage</u></a> to access and download the dataset.</p>
				<p>
				Note that participants will receive the password to access the public testing set after completing the registration form and sending the request to the organisers.
			</p>
			</div>

			<div>
				<p align="justify">  
					<p id="tasks">
						<b><span style="font-size:14.0pt">Tasks</span></b>
					</p>
					<p align="justify"> 
						<!-- During a human-to-robot handover, the robot should be able to determine if a container-like object is filled with a content and, therefore, estimate how heavy the object is based on the amount and type of content (filling). This estimation will enable the robot to apply the right amount of forces when handing the object, avoid to a handover failure or spill content from the object.-->
						Prior to a human-to-robot handover, the robot must determine on-the-fly the weight of a container (e.g. the amount and type of content).
						This estimation will enable the robot to apply the correct force when grasping it, avoiding slippage, crashing the container or spilling its content.
					</p>
					<p align="justify">
						We define the weight of the object handled by the human as the sum of the mass of an unseen container and the mass of the unknown filling within the container. However, we focus the challenge only towards the estimation of the mass of the filling (<b>overall task</b>). To estimate the mass of the filling, we expect a perception system to reason on the capacity of the container and to determine the type and amount of filling present in the container.<br>
					</p>
					<p align="center">
						<img src="resources/diagram_tasks.png" alt="Tasks" style="width:80%">
					</p>
					<p>
						Therefore, we identify three tasks for the participants:
					</p>
					<ul style="padding:0px">
						<li class="clear">
							<b>Task 1 (T1)</b> <i>Filling level classification</i>. Containers can be either empty or filled with an unknown content at 50% or 90% of the whole capacity of the container. For each recording related to a container, the goal is to classify the filling level. There are three classes: 0 (empty), 50, (half full), and 90 (full).
						</li>
						<li class="clear">
							<b>Task 2 (T2)</b> <i>Filling type classification</i>. Containers can be either empty or filled with an unknown content. For each recording related to a container, the goal is to classify the type of filling, if any. There are four filling type classes: 0 (empty), 1 (pasta), 2 (rice), 3 (water).
						</li>
						<li class="clear">
							<b>Task 3 (T3)</b> <i>Container capacity estimation</i>. Containers vary in shape and size. For each recording related to a container, the goal is to estimate the capacity of the container.
						</li>
					</ul>
					<p align="justify">
						For each recording related to a container, we then compute the filling mass estimation using the estimations of filling level from T1, filling type from T2, and container capacity from T3, and using the prior density of each filling type per container. The density of pasta and rice is computed from the annotation of the filling mass, capacity of the container, and filling level for each container. Density of the water is 1 g/mL. The formula selects the annotated density for a container based on the estimated filling type. 
					</p>
			</div>


			<br>
			<div>
				<a name="documentation"></a>
				<p id="documentation">
					<b><span style="font-size:14.0pt">Starting kit and documentation</span></b> 
				</p>
				<p align="justify">
					<i>Evaluation toolkit + script to pre-process the dataset</i><br>
					[<a href="https://github.com/CORSMAL/CORSMALChallengeEvalToolkit" target="_blank"><u>code</u></a>]
					<br><br>
					<i>Vision baseline for CORSMAL Benchmark</i>: a vision-based algorithm, part of a larger system, proposed for localising, tracking and estimating the dimensions of a container with a stereo camera.<br>
					[<a href="https://ieeexplore.ieee.org/document/8968407/" TARGET = "_blank"><u>paper</u></a>][<a href="https://github.com/CORSMAL/Benchmark" target="_blank"><u>code</u></a>][<a href="benchmark.html" target="_blank"><u>webpage</u></a>]
					<br><br>
					<i>LoDE</i>: a method that jointly localises container-like objects and estimates their dimensions with a generative 3D sampling model and a multi-view 3D-2D iterative shape fitting, using two wide-baseline, calibrated RGB cameras.<br>
					[<a href="https://arxiv.org/abs/1911.12354" TARGET = "_blank"><u>paper</u></a>][<a href="https://github.com/CORSMAL/LoDE" target="_blank"><u>code</u></a>][<a href="LoDE.html" target="_blank"><u>webpage</u></a>]
					<br><br>
					<i>Mask R-CNN</i><br>
					[<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" TARGET = "_blank"><u>paper</u></a>][<a href="https://github.com/matterport/Mask_RCNN" target="_blank"><u>code</u></a>]
					<br><br>
					<i>SiamMask</i><br>
					[<a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf" TARGET = "_blank"><u>paper</u></a>][<a href="https://github.com/foolwood/SiamMask" target="_blank"><u>code</u></a>]
					<br><br>
					<i>ResNet-18</i> (network available in PyTorch)<br>
				 [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target = "_blank"><u>paper</u></a>][<a href="https://pytorch.org/" target = "_blank"><u>code</u></a>] 
				 <br><br>
				 	<i>Mask R-CNN + ResNet-18</i>: Vision baseline for filling properties estimation. Independent classification of filling level and filling type using a re-trained ResNet-18 and a single RGB image crop extracted from the most confident instance estimated by Mask R-CNN in the last frame of a video. The baseline works only with glasses and cups, and fails with non-transparent containers (extra class opaque). We refer to this baseline as Mask R-CNN+RN18 in the leadeboard (run for each camera view independently).<br>
					[<a href="" target="_blank"><u>code</u></a>] (COMING SOON)
					<br><br>
				  <i>Additional references</i><br>
				  [<a href="resources/ICPR2020_CORSMAL_Challenge_Additional_References.pdf" target="_blank"><u>document</u></a>]
				</p>
    </div>
				<br>  
				<p id="evaluation">
					<b><span style="font-size:14.0pt">Evaluation</span></b> 
				</p>
				<p id="perfscores">
					<b><span style="font-size:12.0pt">Performance scores</span></b>
				</p>
				<p align="justify">
					For filling level classification (Task 1) and filling type classification (Task 2), the organisers compute the Weighted Average F1-score (WAFS) across the classes, each weighted by the number of recordings in each class. For container capacity estimation (Task 3), the organisers compute the relative absolute error between the estimated and the annotated capacity for each configuration, and then compute the Average Capacity Score (ACS), that is the average score across all the configurations. The score for each configuration is computed as the exponential of the negative relative absolute error. For filling mass estimation, the organisers compute the relative absolute error between the estimated and the annotated filling mass for each configuration, unless the annotated mass is zero (i.e. empty) and then the estimation is set to zero if the estimation is also zero, otherwise equal to the estimation. The organisers then compute the Average filling Mass Score (AMS), that is the average score across all the configurations, as done for the container capacity estimation. If the capacity of the container or the filling mass in one configuration is not estimated (value should be -1), then the score is set to zero.<br><br>
					Note: The final score (AMS) will be weighed based on the number of tasks submitted (i.e. 0.33 for one task, 0.66 for two tasks, 1 for the three tasks). <br><br>
					See the <a href="resources/ICPR2020_CORSMAL_Challenge_PerformanceScores.pdf" target="_blank"><u>document</u></a> for technical details on the performance measures.<br><br>
				</p>
				<p id="submission">
					<b><span style="font-size:12.0pt">Submission guidelines</span></b>
				</p>
				<p align="justify">
					Participants must submit the estimations for each configuration of the public testing set in a <a href="data/ICPR20/submission_form_ICPR20.csv" target = "_blank"><u>csv file</u></a> to <a href="mailto:corsmal-challenge@qmul.ac.uk"><u>corsmal-challenge@qmul.ac.uk</u></a>.
					Each row corresponds to the respective configuration (as object id and sequence id), the third column is the estimated filling level class (0, 50 or 90), the fourth column is the estimated filling type class (0: empty, 1: pasta, 2: rice, 3: water), and the fifth column is the estimated capacity of the container in millilitres. Participants can compete to any or a combination of the 3 tasks. Columns related to tasks not addressed by the participants should be filled with -1 values. Method failures or configurations not addressed should also be filled with -1 values.
				</p>
				<p align="justify">
					Participants should also include in the body of the email:<br>
					- Team name<br>
					- Modalities employed (RGB, IR, Depth, Audio, IMU)<br>
					- Number of (1, 2, 3, 4) and which views participants employed (demonstrator, manipulator, left side-view, right side-view)<br>
					- Completed task(s) (T1, T2, T3)<br>
					- Execution time in seconds for the complete execution of the public test.
				</p>
				<p align="justify">
					Participants will submit the source code and executable files that will be run by the organisers on the private test set to generate the estimations of the same properties for each configuration. 
					The source code should be properly commented and easy-to-run. For achieving so, participants should provide an environment (e.g. docker or conda) that can locally install all the necessary libraries to reproduce and run the code. 
					The organisers will require to input an absolute path to the testing set to perform the evaluation.
					Therefore, participants should prepare the source code in such a way that data path is provided as input argument.
					Also, we recommend participants to have a <i>single</i> README file with a brief description; employed hardware, programming language, and libraries; installation instructions; demo test; running instructions on the testing set; external links to pre-trained models to download, if any; and licence.
				</p>
				<p>
					Note that organisers will run the submitted software with the following specifications:
					<br>
					<i>Hardware</i><br>
					- CentOS Linux release 7.7.1908 (server machine)<br>
					- Kernel: 3.10.0-1062.el7.x86_64<br>
					- GPU: (4) GeForce GTX 1080 Ti<br>
					- GPU RAM: 48 GB<br>
					- CPU: (2) Xeon(R) Silver 4112 @ 2.60GHz<br>
					- RAM: 64 GB<br>
					- Cores: 24<br>
					<i>Libraries</i><br>
					- Anaconda 3 (conda 4.7.12)<br>
					- CUDA 7-10.2<br>
					- Miniconda 4.7.12<br>
				</p>
				<p>
					The source code will be deleted by the organisers after the release of the results.
				</p>
			</p>

			<p id="rules">
							<b><span style="font-size:12.0pt">Rules</span></b> 
						</p> 
						<p align="justify">
							Any individual or research group can download the dataset and participate in the challenge. The only prior knowledge available to the models is the filling types (water, rice, and pasta), the filling levels (empty, 50%, and 90%), and the high-level category of the containers (cup, glass, food box). The organisers do not allow the use of prior 3D object models. Inferences must be generated automatically by a model that uses as input any of the provided data modalities or a combination (for example, video, audio or audio-visual fusion); i.e., non-automatic manipulation of the testing data (e.g., manual selection of frames) is not allowed. The use of additional training data is allowed but the provided testing set cannot be used for training. Models must perform the estimations for each testing sequence only using data from that sequence, and the training set; but not from other sequences. Learning (e.g. model fine tuning) across testing sequences is not allowed.
						</p>

			<div>
				<a name="leaderboard"></a>
			<p align="justify"> 
				<p id="results" class="xmsonormal" style="text-justify:inter-ideograph;margin-bottom:0px"><b><span style="font-size:14.0pt">Leaderboard</span></b></p>
				<p align="justify">
					The evaluation is based on the results of both testing sets and participants will be ranked across all configurations. 
					<!-- Additionally, the organisers will provide the ranks for each task independently.  -->
					To calculate the estimation of the filling mass (overall task), the organisers use results from the random case if one (or two) of the tasks is (are) not submitted by a participant team.
				</p>
				<p>
					<u>Note</u>: The score for the overall task is not a linear combination of the scores outputted for Task 1, Task 2, and Task 3, but it takes into consideration the formula for computing the filling mass based on the estimations of each task for each configuration. This means that a method with lower Task 1, Task 2 and Task 3 scores can obtain a higher Overall score compared to other methods because the performance on each run is more accurate in general.<br>
					More details in the technical <a href="resources/ICPR2020_CORSMAL_Challenge_PerformanceScores.pdf" target="_blank"><u>document</u></a>.
				</p>
				<p>
					The organisers will declare the winner of the challenge based on the score on the overall task.
					The best-performing entries will be presented at the conference venue.
					Selected participants will be invited to co-author a paper to discuss and analyse the research outcomes of the challenge.
				</p>
				<p align="center">
					<b>Overall task: Filling mass estimation</b>
				</p>
				<div class="example-table" id="task4_table"></div>
				<script type="text/javascript">
					var table = new Tabulator("#task4_table",
					{
						// height:"150px",
						placeholder:"No Data Available",
						maxHeight:"100%",
						layout:"fitColumns",      //fit columns to width of table
						reactiveData:true, //turn on data reactivity
						data:tabledatatask4, //load data into table
						columns:[
						{title:"Team", field:"team"},
						{title:"Description", field:"desc", width:370, align:"left", formatter:"textarea", headerSort:false},
						{title:"Task 1", field:"task1", width:60, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
						{title:"Task 2", field:"task2", width:60, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
						{title:"Task 3", field:"task3", width:60, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
						{title:"Public", field:"public", width:80, sorter:"number", align:"right"},
						{title:"Private", field:"private", width:80, sorter:"number", align:"right"},
						{title:"Overall", field:"combined", width:80, sorter:"number", align:"right"},
						],
						initialSort:[
							{column:"combined", dir:"desc"}, //sort by this first
							]
						});
					</script>
				<br>	
				<p align="center">
					<b>Task 1: Filling level classification</b>
				</p>
				<div class="example-table" id="task1_table"></div>
				<script type="text/javascript">
					var table = new Tabulator("#task1_table",
					{
						// height:"150px",
						placeholder:"No Data Available",
						maxHeight:"100%",
						layout:"fitColumns",      //fit columns to width of table
						reactiveData:true, //turn on data reactivity
						data:tabledatatask1, //load data into table
						columns:[
						{title:"Team", field:"team"},
						{//create column group
	            title:"Input modalities",
	            columns:[
	            {
	            	title:"",
	            	columns:[
	            	{title:"A", field:"audio", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
	            	]
	            },
	            
							{//create column group
	            	title:"View 1",
	            	columns:[
	            		{title:"RGB", field:"rgb1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir1",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 2",
	            	columns:[
	            		{title:"RGB", field:"rgb2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir2",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 3",
	            	columns:[
	            		{title:"RGB", field:"rgb3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir3",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 4",
	            	columns:[
	            		{title:"RGB", field:"rgb4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		        	}, 
	            ],
	        	},
						{title:"Public", field:"public", width:80, sorter:"number", align:"right"},
						{title:"Private", field:"private", width:80, sorter:"number", align:"right"},
						{title:"Overall", field:"combined", width:80, sorter:"number", align:"right"},
						],
						initialSort:[
							{column:"combined", dir:"desc"}, //sort by this first
							]
						});
					</script>
				<p align="center">
					<b>Task 2: Filling type classification</b>
				</p>
				<div class="example-table" id="task2_table"></div>
				<script type="text/javascript">
					var table = new Tabulator("#task2_table",
					{
						// height:"150px",
						placeholder:"No Data Available",
						maxHeight:"100%",
						layout:"fitColumns",      //fit columns to width of table
						reactiveData:true, //turn on data reactivity
						data:tabledatatask2, //load data into table
						columns:[
						{title:"Team", field:"team"},
						{//create column group
	            title:"Input modalities",
	            columns:[
	            {
	            	title:"",
	            	columns:[
	            	{title:"A", field:"audio", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
	            	]
	            },
	            
							{//create column group
	            	title:"View 1",
	            	columns:[
	            		{title:"RGB", field:"rgb1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir1",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 2",
	            	columns:[
	            		{title:"RGB", field:"rgb2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir2",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 3",
	            	columns:[
	            		{title:"RGB", field:"rgb3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir3",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 4",
	            	columns:[
	            		{title:"RGB", field:"rgb4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		        	}, 
	            ],
	        	},
						{title:"Public", field:"public", width:80, sorter:"number", align:"right"},
						{title:"Private", field:"private", width:80, sorter:"number", align:"right"},
						{title:"Overall", field:"combined", width:80, sorter:"number", align:"right"},
						],
						initialSort:[
							{column:"combined", dir:"desc"}, //sort by this first
							]
						});
					</script>
				<p align="center">
					<b>Task 3: Container capacity estimation</b>
				</p>
				<div class="example-table" id="task3_table"></div>
				<script type="text/javascript">
					var table = new Tabulator("#task3_table",
					{
						// height:"150px",
						placeholder:"No Data Available",
						maxHeight:"100%",
						layout:"fitColumns",      //fit columns to width of table
						reactiveData:true, //turn on data reactivity
						data:tabledatatask3, //load data into table
						headerHozAlign:"center",
						columns:[
						{title:"Team", field:"team"},
						{//create column group
	            title:"Input modalities",
	            columns:[
	            {
	            	title:"",
	            	columns:[
	            	{title:"A", field:"audio", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
	            	]
	            },
	            
							{//create column group
	            	title:"View 1",
	            	columns:[
	            		{title:"RGB", field:"rgb1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth1", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir1",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 2",
	            	columns:[
	            		{title:"RGB", field:"rgb2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth2", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir2",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 3",
	            	columns:[
	            		{title:"RGB", field:"rgb3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth3", width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir3",   width:40, headerHozAlign:"center", align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		          },
		          {//create column group
		            title:"View 4",
	            	columns:[
	            		{title:"RGB", field:"rgb4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"D", field:"depth4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
									{title:"IR", field:"ir4", width:40, align:"center", formatter:"tickCross", formatterParams:{allowEmpty:true,}, headerSort:false},
		            ],
		        	}, 
	            ],
	        	},
						{title:"Public", field:"public", width:80, sorter:"number", align:"right"},
						{title:"Private", field:"private", width:80, sorter:"number", align:"right"},
						{title:"Overall", field:"combined", width:80, sorter:"number", align:"right"},
						],
						initialSort:[
							{column:"combined", dir:"desc"}, //sort by this first
							]
						});
					</script>
					<p align="justify">
						Legend:<br>
						- View 1: view from the fixed camera on the left side of the manipulator<br>
						- View 2: view from the fixed camera on the right side of the manipulator<br>
						- View 3: view from the fixed camera mounted on the manipulator (robot)<br>
						- View 4: view from the moving camera worn by the demonstrator (human)<br>
						- A: audio modality<br>
						- RGB: colour data<br>
						- D: depth data<br>
						- IR: infrared data from narrow-baseline stereo camera<br>
						- ZCR: Zero-crossing rate<br>
						- MFCC: Mel-frequency cepstrum coefficients<br>
						- ZCR: Zero-crossing rate<br>
						- A5F: Audio 5 features (MFCC, chromogram, mel-scaled spectrogram, spectral contrast, tonal centroid)<br>
						- kNN: k-Nearest Neighbour classifier<br>
						- SVM: Support Vector Machine classifier<br>
						- RF: Random Forest classifier<br>
						- PCA: Principal component analysis<br>
					</p>
				</p>
				<br>
			</div>

			<div style="height:auto;width:100%;overflow:hidden;">
					<p id="results" class="xmsonormal" style="text-justify:inter-ideograph;margin-bottom:0px"><b><span style="font-size:14.0pt">Submissions</span></b></p>
					Select a team to see the details<br>
					<div style="width:200px; float:left;">
						<!-- <b>Team name</b><br> -->
						<ul id="teams">
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('concatenation_metadata')">Concatenation</a></li>
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('hvrl_metadata')">HVRL</a></li>
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('bit_metadata')">Because It's Tactile</a></li>
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('ntnuerc_metadata')">NTNU-ERC</a></li>
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('challengers_metadata')">Challengers</a></li>
						  <li><a class="" title="Show metadata" onclick="AppearMetadata('acc_metadata')">ACC</a></li>
						</ul>
        	</div>
        	<div style="margin-left:220px;" >
        			<!-- <b>Metadata</b><br> -->
        			<table class="metadata" style="border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td style="text-align:left;border: none;"></td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:left;border: none;"></td>
				        	</tr>
        					<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td style="text-align:left;border: none;"></td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td style="text-align:left;border: none;"></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td style="text-align:left;border: none;"></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td style="text-align:left;border: none;"></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td style="text-align:left;border: none;"></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td style="text-align:left;border: none;"></td>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="concatenation_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="4" style="text-align:left;border: none;">Concatenation</td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/QiLiu.jpg" alt="" width="75px" /><br>Qi Liu</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/ChuanlinLan.jpg" alt="" width="75px" /><br>Chuanlin Lan</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/FanFeng.jpg" alt="" width="75px" /><br>Fan Feng</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/RosaChan.jpg" alt="" width="75px" /><br>Rosa Chan</td>
				        	</tr>
        					<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio and RGB from all views. Integrate the audio feature learning and the knowledge of container categories via the object detection pre-trained model.</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio and RGB from all views. Integrate the audio feature learning and the knowledge of container categories via the object detection pre-trained model.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td colspan="4" style="text-align:left;border: none;">RGB from all views. Sample from the shape distribution based on the prior of container categories</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td colspan="4" style="text-align:left;border: none;">The solution is divided into three folds to help the agent shape a rich understanding of the pouring procedure. First, the agent obtains the prior of container categories (cup, glass, box) through the object detection framework. Second, audio features are integrated with the prior to make the agent learn a multi-modal feature space. Finally, the agent infers the distribution of both the container capacity and fluid properties.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://link.springer.com/chapter/10.1007/978-3-030-68793-9_33"><u>VA2Mass: Towards the Fluid Filling Mass Estimation via Integration of Vision & Audio</u></a></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="hvrl_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="4" style="text-align:left;border: none;">HVRL</td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/ReinaIshikawa.JPG" alt="" width="75px" /><br>Reina Ishikawa</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/YuichiNagao.jpg" alt="" width="75px" /><br>Yuichi Nagao</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Ryo Hachiuma</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Hideo Saito</td>
				        	</tr>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio. From the prediction model for Task2, intermediate features are extracted and pass through LSTM models.</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio. Raw audio waveform converted into a log-Mel spectrogram that is cropped into a fixed-size and provided as input to convolutional neural network model with a VGG backbone.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td colspan="4" style="text-align:left;border: none;">RGB + Depth from view 1: fixed camera on the left side of the manipulator (robot). Mask-RCNN detects the target object (silhouette) and a point cloud is obtained from a selected frame in the video. The volume of the container is then computed by approximating the object shape as a cuboid from the point cloud.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://link.springer.com/chapter/10.1007/978-3-030-68793-9_32"><u>Audio-Visual Hybrid Approach for Filling Mass Estimation</u></a></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://github.com/YuichiNAGAO/ICPRchallenge2020"><u>https://github.com/YuichiNAGAO/ICPRchallenge2020</u></a></td>
        					</tr>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="bit_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="4" style="text-align:left;border: none;">Because It's Tactile</td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/Vladimir_150x165.jpg" alt="" width="75px" /><br>Vladimir Iashin</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/Francesca_150x165.jpg" alt="" width="75px" /><br>Francesca Palermo</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/Gokhan_150x165.jpg" alt="" width="75px" /><br>Gokhan Solak</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/Claudio_150x165.jpg" alt="" width="75px" /><br>Claudio Coppola</td>
				        	</tr>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio + RGB from all views. GRU(VGGish) + GRU(R(2+1)d [RGB-only]) for each view, and RandomForest(classical audio features)</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td colspan="4" style="text-align:left;border: none;">Audio. GRU(VGGish) and and RandomForest</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td colspan="4" style="text-align:left;border: none;">RGB + IR + Depth (left-side view). LoDE on detector's predictions; if no object was detected, use of the train set's average</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td colspan="4" style="text-align:left;border: none;">We sum up logits from all four views obtained from GRU on top of R(2+1)d features to form one prediction for each event, which are, then, averaged with the GRU output on top of VGGish features, and RandomForest predictions on top of 30+ classical audio features (eg mfccs, chromagram, energy, spread). LoDE with Mask R-CNN for object detection (glass, bottle, or book for boxes), in frame 1 and 20 of the videos (view 1, RGB-D-IR) to estimate container capacity. Average of the train set is used if no detection.</td>
        						<!-- R(2+1)d feature extraction followed by GRU for each RGB view, and sum of the logits. VGGish feature extraction Average -->
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://link.springer.com/chapter/10.1007/978-3-030-68793-9_31" TARGET = "_blank"><u>Top-1 CORSMAL Challenge 2020 submission: Filling mass estimation using multi-modal observations of human-robot handovers</u></a></td>
        					</tr>
<!--         									        	<tr>
        						<td style="text-align:left;border: none;">ArXiv:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://arxiv.org/pdf/2012.01311.pdf"><u>https://arxiv.org/pdf/2012.01311.pdf</u></a></td>
        					</tr> -->
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td colspan="4" style="text-align:left;border: none;"><a href="https://github.com/v-iashin/CORSMAL"><u>https://github.com/v-iashin/CORSMAL</u></a></td>
        					</tr>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="ntnuerc_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="2" style="text-align:left;border: none;">NTNU-ERC</td>
				        	</tr>
									<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Guilherme Christmann</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Jyun-Ting Song</td>
				        	</tr>
        					<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td colspan="2" style="text-align:left;border: none;">N/A</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td colspan="2" style="text-align:left;border: none;">Audio</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td colspan="2" style="text-align:left;border: none;">Depth from view 3: the fixed camera mounted on the manipulator (robot)</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td colspan="2" style="text-align:left;border: none;">Extraction of 40 normalized MFCC features in a window size of 20 ms at 22 kHz, with a maximum length of 30 s, and zero-padding to preserve the same duration across audio data. Filling type classification with a neural network consisting of 2 convolutional layers and 1 linear layer. Regression of the container capacity by extracting a region of interest (ROI) around the object localised in the depth data (view 3) and providing the ROI and its size to a neural network (4 convolutional-batchnorm followed by 3 linear layers). The size of the ROI is concatenated to the feature between the 2nd and 3rd linear layer. Only detections/ROIs up to 700 mm far from the camera, while processing the video backwards, are considered (prior knowledge that the person will extend the arm towards the robot). The closest contour is selected, if multiple detections in a frame.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td colspan="2" style="text-align:left;border: none;"><a href="resources/challenge/2020.11.30_CORSMAL_NTNU-ERC_Report.pdf"><u>NTNU-ERC Report</u></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td colspan="2" style="text-align:left;border: none;"><a href="https://github.com/guichristmann/CORSMAL-Challenge-2020-Submission"><u>https://github.com/guichristmann/CORSMAL-Challenge-2020-Submission</u></td>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="challengers_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="4" style="text-align:left;border: none;">Challengers</td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Neeharika</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Krishna</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Bakhtawar</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Dinesh</td>
				        	</tr>
        					<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Audio</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Audio</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td  colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Sound-based classification of filling type and level: After suppressing the noise in each audio signal via spectral gating, the absolute value of the Short-Time Fourier Transform (STFT) is extracted as input feature for a classifier based on a 5-layer fully connected neural network, trained with Adam optimizer and dropout on the last layer to reduce overfitting.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td  colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td  colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
        				</tbody>
        			</table>
        			<table class="metadata" id="acc_metadata" style="display:none;border: none;">
        				<tbody>
        					<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team name:</td>
        						<td colspan="4" style="text-align:left;border: none;">Organisers</td>
				        	</tr>
				        	<tr>
        						<tr>
        						<td style="text-align:left;border: none;width:100px">Team members:</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Santiago Donaher</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Alessio Xompero</td>
        						<td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Andrea Cavallaro</td>
        						<!-- <td style="text-align:center;border: none;"><img src="images/ICPR2020/participants/avatar.png" alt="" height="82px" /><br>Dinesh</td> -->
				        	</tr>
        					<tr>
        						<td style="text-align:left;border: none;">Task 1:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Audio</td>
				        	</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 2:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Audio</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Task 3:</td>
        						<td  colspan="4" style="text-align:left;border: none;">N/A</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Summary:</td>
        						<td  colspan="4" style="text-align:left;border: none;">Sound-based model that first identifies the action performed by a person with a container and then determines the amount and type of content using an action-specific  classifier. The models consists of three independent CNN classifiers and combines content types and levels into a set of seven feasible classes. Task 1 and Task 2 are jointly performed by the model.</td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Paper:</td>
        						<td  colspan="4" style="text-align:left;border: none;"><a href="https://arxiv.org/pdf/2103.15999.pdf"><u>https://arxiv.org/pdf/2103.15999.pdf</u></a></td>
        					</tr>
				        	<tr>
        						<td style="text-align:left;border: none;">Code:</td>
        						<td  colspan="4" style="text-align:left;border: none;"><a href="https://github.com/CORSMAL/ACC/"><u>https://github.com/CORSMAL/ACC/</u></a></td>
        					</tr>
        				</tbody>
        			</table>
					</div>
				</div>
				<br>

			
			</div>

			<div>
				<p id="organisers">
					<b><span style="font-size:14.0pt">Organisers</span></b>
				</p>
				<p align="justify">
					Alessio Xompero, Queen Mary University of London (UK)<br>
					Andrea Cavallaro, Queen Mary University of London (UK)<br>
					Apostolos Modas, École polytechnique fédérale de Lausanne (Switzerland)<br>
					Aude Billard, École polytechnique fédérale de Lausanne (Switzerland)<br>
					Dounia Kitouni, Sorbonne Université (France)<br>
					Kaspar Althoefer, Queen Mary University of London (UK)<br>
					Konstantinos Chatzilygeroudis, University of Patras (Greece)<br>
					Nuno Ferreira Duarte, École polytechnique fédérale de Lausanne (Switzerland)<br>
					Pascal Frossard, École polytechnique fédérale de Lausanne (Switzerland)<br>
					Ricardo Sanchez-Matilla, Queen Mary University of London (UK)<br>
					Riccardo Mazzon, Queen Mary University of London (UK) <br>
					Véronique Perdereau, Sorbonne Université (France)<br>
				</p>
			</div>

			
		</div>
	</div>
	<!-- ####################################################################################################### -->
<!-- Begin of Sponsors and Partners div -->
<div class="wrapper row1">
	<div id="header" class="clear" style="padding: 0px 0px 0px 20px;width:920px">
		<div class="fl_left" style="margin-top: 0px;">
			<ul>
				<li style="padding: 0 0 0 0">
					<p>
						<b>Sponsors</b>
						<br>
						<br>
					</p>
					<p>
						<a href="http://www.chistera.eu" TARGET = "_blank"><img src="images/chist-era_logo_crop.png" style="height:35px;" alt="Chistera logo"/></a>
						<a href="https://epsrc.ukri.org/" target="_blank"><img src="images/EPSRC_logo.png" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="EPSRC logo"></a>
						<a href="http://www.agence-nationale-recherche.fr/en/" target="_blank"><img src="images/ANR_logo.png" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="ANR logo"></a>
						<a href="http://www.snf.ch/en/Pages/default.aspx" target="_blank"><img src="images/FNS_logo.jpg" style="padding:10px 0px 0px 10px;height:35px;margin-bottom: 10px;" alt="FNS logo"></a>
					</p>
				</li>
			</ul>
		</div>
		<div class="fl_right" style="margin-top: 0px;">
			<ul style="margin-bottom: 0px;">
				<li style="margin: 0px 4px 0px 0; padding: 0 0px 0 0;">
					<p>
						<b>Partners</b>
						<br>
						<br>
					</p>
					<p>
						<a href="https://www.qmul.ac.uk/" target="_blank"><img src="images/QMUL_logo.jpg" alt="Queen Mary University of London" style="padding:5px 10px 0px 0px;height:45px;"></a>
						<a href="http://www.sorbonne-universite.fr/en" target="_blank">
							<img src="images/Sorbonne_University_logo.png" alt="Sorbonne University" style="padding:5px 10px 0px 0px;height:45px;">
						</a>
						<a href="https://www.epfl.ch/en/home/" target="_blank">
							<img src="images/EPFL_logo.png" alt="EPFL" style="padding:5px 0px 0px 0px;height:45px;">
						</a>
					</p>
				</li>
			</ul>
		</div>
	</div>
</div>
<!-- End of Sponsors and Partners div -->
<br><br>
<!-- ####################################################################################################### -->
<!-- <div class="clearing">&nbsp;</div>-->
<div class="wrapper bottomPg">
	<div id="copyright">
		<div id="fl_left" style="font-size:12px">
			© Copyright CORSMAL 2019-2022
		</div>	  
	</div>
</div>
<!-- footer -->
<!--added to clear error with content element -->
</body>
</html>
